# -*- coding: utf-8 -*-
"""diabetic_retinopathy

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VJbnbNOCjxEYzdzUM2MFijjzbyMFV8H6

# Downloading Dataset
"""

import requests
url = "https://storage.googleapis.com/kaggle-data-sets/131128/418031/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230902%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230902T051522Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=596dbf5a71c0060f81c957f2bd9fed04dd007a3503fdfa8e7cc229b4fd4e27feeb09f14e0ded9e8573c51b4deb26913b777e169bf56dfa4761a6a2a3cbbd1b741f949a7bb3d016449fe63dca7e8ca9c17fbe65a60ee35c9beb1f1e69ed8f9224603cc4de58be4e0c3e981909f960ab53595cfdec4a96a0f05d3c27a7dd86373cefbee50a18715b565e43ba4b067d8161ee6348c5766ad4e504aa9e08e285bbe062c624d8663bbc25471828ca7ec909e6579b99505c6c1c8517a5320912367b2b0e42cf8b1c93f7a0d6dc72e7882e6d95f8fd70aed7b28d805772ecc5707003b4631b47c34ca92a0823253cb8361242dec87d1993438c69ab875aac22ab171400"
url2="https://storage.googleapis.com/kaggle-data-sets/505422/952963/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230904%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230904T184901Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=7d166e04c4e35ac5183163bdb983a5ee3b714294a03e71e8750157e78b899a755668bc791e19adc88304629f9af835709a51946b81dea22591e09865cc1d9141138a1d86f85a1b4bd34946767f0245d5a43c620da651b3d4fc2c8f0561a18c727bd58d8166ae482cdb2a89b88c9d52fa18f42bfed7b473184239c15626895eb8775290b15b022091646a64528fd1d37a6b7522781aab7668cb71ab6487a9fc08d4fd7a25d5417b896e3613a5bc1d36752fad32356e3d66d2831f84131810a09ca58778ea0505300d78648fc7d7e87a8d8288d8b959a7642cd5c18d244aef0e8810685f91ebd3eab3614064fb9d7f7e446b7be726714067833030bda328fbec3f"
chunk_size = 1024  # Adjust this to your preferred chunk size

response = requests.get(url2, stream=True)
with open("dataset.zip", "wb") as file:
    for chunk in response.iter_content(chunk_size=chunk_size):
        if chunk:
            file.write(chunk)

import os
os.makedirs("dataset",exist_ok=True)

import zipfile

with zipfile.ZipFile("dataset.zip", "r") as zip_ref:
    zip_ref.extractall("dataset")

del response

!mv dataset/train.csv -T train.csv

"""# importing"""

import os,time
from tqdm import tqdm

import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import layers

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
            print(gpu)
    except RuntimeError as e:
        print(e)

"""# Loading Dataset"""

def load_one_image(path,label):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image)
    return image,label

from sklearn.model_selection import train_test_split
image_names = [os.path.join(root, fl) for root, dirs, files in os.walk("dataset") for fl in files]
np.random.shuffle(image_names)

# Extract and encode labels
labels = [0 if i.split("/")[-2] =="No_DR" else 1 for i in image_names]
# label_set = set(labels)
# label_to_index = {label: idx for idx, label in enumerate(label_set)}
# num_classes = len(label_set)
# label_indices = [label_to_index[label] for label in labels]
# labels_one_hot = tf.one_hot(label_indices, depth=num_classes)

# # Convert to NumPy arrays
# image_names = np.array(image_names)
# labels = labels_one_hot.numpy()  # Convert TensorFlow tensor to NumPy array

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(image_names,labels,random_state=15,test_size=0.2)

AUTOTUNE = tf.data.AUTOTUNE
train_dataset=tf.data.Dataset.from_tensor_slices((X_train,y_train)).map(load_one_image,num_parallel_calls=AUTOTUNE)
test_dataset=tf.data.Dataset.from_tensor_slices((X_test,y_test)).map(load_one_image,num_parallel_calls=AUTOTUNE)
train_dataset = train_dataset.batch(batch_size=16).prefetch(AUTOTUNE)
test_dataset = test_dataset.batch(batch_size=16).prefetch(AUTOTUNE)

"""# Model Building"""

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.applications import ResNet50

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

def build_model():
  intermediate_layer = base_model.get_layer('conv3_block2_1_conv')

  x = Conv2D(32, (3, 3), padding="same", activation="relu")(intermediate_layer.output)
  x = MaxPooling2D((2, 2))(x)
  x = Flatten()(x)
  x = Dense(224, activation='relu', name='dense_1')(x)
  x = Dense(100, activation='relu', name='dense_3')(x)

  output = Dense(2, activation='sigmoid', name='output')(x)
  for layer in base_model.layers:
      layer.trainable = False
  model = tf.keras.Model(inputs=base_model.input, outputs=output)
  return model
model=build_model()
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Print a summary of your model
model.summary()

model_checkpoint=tf.keras.callbacks.ModelCheckpoint("checkpoint.h5",save_best_only=True)
tensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir="log",histogram_freq=1)
history=model.fit(train_dataset,epochs=10,validation_data=test_dataset,callbacks=[model_checkpoint,tensorboard_callback])

model.save("/content/drive/MyDrive/Projects_DSVV/Diabetic_Retinopathy/bin_model.h5")

